{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgW1bZE0NGkvurc8jmGSnL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irawan09/PySpark/blob/main/PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIkbM2kGaibZ"
      },
      "source": [
        "!git clone 'https://github.com/irawan09/PySpark.git'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KGJjKdBSbOE"
      },
      "source": [
        "#PySpark in Google Colab\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqOnUwlqTIJh"
      },
      "source": [
        "## Setting the Environment\n",
        "Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaUi-GyFTY9a"
      },
      "source": [
        "### Installing Java on the virtual environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6tfnaviHsqs"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeCGJVodTj9a"
      },
      "source": [
        "### Download and Setting JAVA and Spark Home directory\n",
        "Next, we will install Apache Spark 3.0.1 with Hadoop 2.7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCJVHskIHx-V"
      },
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CCYAvHxUINZ"
      },
      "source": [
        "Now, we just need to unzip that folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnR04zKkH0_S"
      },
      "source": [
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjOs9RMbUO8J"
      },
      "source": [
        "*Note â€“ At the time of writing this code, I am choosing version 3.0.1. You can change the version depends on you preferences. You can refer on this website : https://archive.apache.org/dist/spark/spark-3.0.1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oCcunnAUVfB"
      },
      "source": [
        "Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjZxuHqaIDH2"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OS4ecgxT0jZ"
      },
      "source": [
        "### Install PySpark and findspark\n",
        "\n",
        "There is one last thing that we need to install and that is the findspark library and PySpark library. It will locate Spark on the system and import it as a regular library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAReOSMyeFzK"
      },
      "source": [
        " !pip install pyspark\n",
        " !pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLwxhKt2Vanv"
      },
      "source": [
        "We need to locate Spark in the system. For that, we import findspark and use the **findspark.init()** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4ksx5fgIHDS"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l4HBHX2Vjr3"
      },
      "source": [
        "If you want to know the location where Spark is installed, use **findspark.find()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piHj_oMTI3ht"
      },
      "source": [
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzjjTazoVsav"
      },
      "source": [
        "###Starting Spark Session\n",
        "\n",
        "SparkSession is the entry point to Spark SQL. It is one of the very first objects you create while developing a Spark SQL application.\n",
        "\n",
        "As a Spark developer, you create a SparkSession using the **SparkSession.builder** method (that gives you access to Builder API that you use to configure the session).\n",
        "\n",
        "Once created, SparkSession allows for creating a DataFrame (based on an RDD or a Scala Seq), creating a Dataset, accessing the Spark SQL services (e.g. ExperimentalMethods, ExecutionListenerManager, UDFRegistration), executing a SQL query, loading a table and the last but not least accessing DataFrameReader interface to load a dataset of the format of your choice (to some extent)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e286P-vXeh73"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Colab\").config('spark.ui.port', '4050').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa_NrYxtJFg5"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REqwFmMsWmqs"
      },
      "source": [
        "If you want to view the Spark UI, you would have to include a few more lines of code to create a public URL for the UI page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSBlhhPmJUw2"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiS1n8nRWo1R"
      },
      "source": [
        "##Roll the Party !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhoqS51OeO4Q"
      },
      "source": [
        "import findspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import Window, Row\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y3wHbjDfmc2"
      },
      "source": [
        "df_matches = spark.read.format('csv').options(header='true').load('/content/PySpark/Matches.csv')\n",
        "df_matches.limit(5).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh4ScHtikqZW"
      },
      "source": [
        "old_cols = df_matches.columns[-3:]\n",
        "new_cols = [\"HomeTeamGoals\", \"AwayTeamGoals\", \"FinalResult\"]\n",
        "old_new_cols = [*zip(old_cols, new_cols)]\n",
        "for old_col, new_col in old_new_cols:\n",
        "    df_matches = df_matches.withColumnRenamed(old_col, new_col)\n",
        "\n",
        "df_matches.limit(5).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxt6rup1iwEc"
      },
      "source": [
        "print(\"Number of row : \", df_matches.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0fjgMD5jcq9"
      },
      "source": [
        "Sometimes you might want to view some specific columns from the dataframe. For those purposes, you can leverage the capabilities of Sparkâ€™s SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muSNhM0Cji6k"
      },
      "source": [
        "df_matches.select(\"Season\",\"HomeTeam\",\"AwayTeam\").show(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBCrFB1Hj7Dp"
      },
      "source": [
        "df_matches = df_matches \\\n",
        "    .withColumn('HomeTeamWin', when(col('FinalResult') == 'H', 1).otherwise(0)) \\\n",
        "    .withColumn('AwayTeamWin', when(col('FinalResult') == 'A', 1).otherwise(0)) \\\n",
        "    .withColumn('GameTie', when(col('FinalResult') == 'D', 1).otherwise(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9o2cqRRlBG-"
      },
      "source": [
        "Bundesliga is a D1 division and we are interested in season <= 2010 and >= 2000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ofO9eqk9OP"
      },
      "source": [
        "bundesliga = df_matches \\\n",
        "                    .filter((col('Season') >= 2000) & \n",
        "                            (col('Season') <= 2010) & \n",
        "                            (col('Div') == 'D1'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_uzbVaKlSRc"
      },
      "source": [
        "Home team features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTkRRQR7lOiQ"
      },
      "source": [
        "home = bundesliga.groupby('Season', 'HomeTeam') \\\n",
        "       .agg(sum('HomeTeamWin').alias('TotalHomeWin'),\n",
        "            sum('AwayTeamWin').alias('TotalHomeLoss'),\n",
        "            sum('GameTie').alias('TotalHomeTie'),\n",
        "            sum('HomeTeamGoals').alias('HomeScoredGoals'),\n",
        "            sum('AwayTeamGoals').alias('HomeAgainstGoals')) \\\n",
        "       .withColumnRenamed('HomeTeam', 'Team')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJOiHDWalVjr"
      },
      "source": [
        "Away game features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwu_8KnNlWK7"
      },
      "source": [
        "away =  bundesliga.groupby('Season', 'AwayTeam') \\\n",
        "       .agg(sum('AwayTeamWin').alias('TotalAwayWin'),\n",
        "            sum('HomeTeamWin').alias('TotalAwayLoss'),\n",
        "            sum('GameTie').alias('TotalAwayTie'),\n",
        "            sum('AwayTeamGoals').alias('AwayScoredGoals'),\n",
        "            sum('HomeTeamGoals').alias('AwayAgainstGoals'))  \\\n",
        "       .withColumnRenamed('AwayTeam', 'Team')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC5nfzlmlgm6"
      },
      "source": [
        "Season features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9iGY6IglfGT"
      },
      "source": [
        "window = ['Season']\n",
        "window = Window.partitionBy(window).orderBy(col('WinPct').desc(), col('GoalDifferentials').desc())\n",
        "table = home.join(away, ['Team', 'Season'],  'inner') \\\n",
        "    .withColumn('GoalsScored', col('HomeScoredGoals') + col('AwayScoredGoals')) \\\n",
        "    .withColumn('GoalsAgainst', col('HomeAgainstGoals') + col('AwayAgainstGoals')) \\\n",
        "    .withColumn('GoalDifferentials', col('GoalsScored') - col('GoalsAgainst')) \\\n",
        "    .withColumn('Win', col('TotalHomeWin') + col('TotalAwayWin')) \\\n",
        "    .withColumn('Loss', col('TotalHomeLoss') + col('TotalAwayLoss')) \\\n",
        "    .withColumn('Tie', col('TotalHomeTie') + col('TotalAwayTie')) \\\n",
        "    .withColumn('WinPct', round((100* col('Win')/(col('Win') + col('Loss') + col('Tie'))), 2)) \\\n",
        "    .drop('HomeScoredGoals', 'AwayScoredGoals', 'HomeAgainstGoals', 'AwayAgainstGoals') \\\n",
        "    .drop('TotalHomeWin', 'TotalAwayWin', 'TotalHomeLoss', 'TotalAwayLoss', 'TotalHomeTie', 'TotalAwayTie') \\\n",
        "    .withColumn('TeamPosition', rank().over(window)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdBYU8KulrRr"
      },
      "source": [
        "table_df = table.filter(col('TeamPosition') == 1).orderBy(asc('Season')).toPandas()\n",
        "table_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}